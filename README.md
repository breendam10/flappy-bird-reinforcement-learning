# flappy-bird-reinforcement-learning üê¶‚ö° ‚Äî Conceitos e design

_Esta documenta√ß√£o explica os conceitos e as decis√µes de design por tr√°s deste projeto_ ‚Äî uma prova de conceito de Aprendizado por Refor√ßo (Reinforcement Learning) aplicada ao jogo Flappy Bird. O objetivo aqui √© esclarecer a ideia, os fundamentos te√≥ricos e as escolhas implementacionais adotadas, **n√£o fornecer um tutorial passo a passo de execu√ß√£o**.

## Vis√£o geral do projeto üöÄ

O projeto demonstra como modelar o problema do Flappy Bird como um Processo de Decis√£o de Markov (MDP) simples e aplicar uma vers√£o tabular de Q-Learning ‚Äî uma implementa√ß√£o direta da Equa√ß√£o de Bellman ‚Äî para estimar as fun√ß√µes de valor `V(s)` e `Q(s,a)`. O agente observa um vetor cont√≠nuo do ambiente, discretiza esse vetor para indexar tabelas (`V` e `Q`) e aprende com atualiza√ß√µes online a cada passo (temporal-difference). 

Embora seja uma prova de conceito, o c√≥digo inclui pontos importantes para estudos: escolha de discretiza√ß√£o, pol√≠tica Œµ-greedy, armazenamento da melhor run para replay, m√©tricas de avalia√ß√£o por epis√≥dio e visualiza√ß√µes b√°sicas para analisar converg√™ncia. ‚úÖ

## Conceitos-chave üìö

- **Processo de Decis√£o de Markov (MDP)**: formaliza o problema como estados `S`, a√ß√µes `A`, recompensas `R`, fator de desconto `Œ≥` e transi√ß√µes probabil√≠sticas `P(s'|s,a)`. O objetivo √© encontrar uma pol√≠tica `œÄ(a|s)` que maximize o retorno esperado.
- **Equa√ß√£o de Bellman**: rela√ß√£o recursiva entre o valor de um estado e os valores dos estados futuros. Para Q-Learning usamos a forma temporal-difference:

```
Q(s,a) ‚Üê Q(s,a) + Œ± [r + Œ≥ max_a' Q(s',a') ‚àí Q(s,a)]
```

onde `Œ±` √© a taxa de aprendizado e `Œ≥` √© o fator de desconto.
- **Q-Learning**: algoritmo _off-policy_ que estima a fun√ß√£o Q (estado-a√ß√£o) sem exigir um modelo do ambiente. A cada transi√ß√£o `(s,a,r,s')` atualizamos Q usando o TD-target `r + Œ≥ max_a' Q(s',a')`.
- **Explora√ß√£o vs Explora√ß√£o (Œµ-greedy)**: a pol√≠tica mistura explora√ß√£o aleat√≥ria com explora√ß√£o da melhor a√ß√£o atual; `Œµ` decai ao longo do tempo (epsilon decay) para transicionar de explora√ß√£o para explora√ß√£o dirigida.
- **Discretiza√ß√£o de estados**: quando observa√ß√µes s√£o cont√≠nuas, para usar uma abordagem tabular precisamos mapear observa√ß√µes a estados discretos. A discretiza√ß√£o define a capacidade de generaliza√ß√£o do agente.

## Principais escolhas deste projeto üéØ

- **Ambiente**: `FlappyBird-v0` (via `flappy_bird_gymnasium`) com `use_lidar=False` ‚Äî observa√ß√µes cont√≠nuas obtidas do ambiente.
- **A√ß√µes**: bin√°rias ‚Äî `0` (n√£o bater) e `1` (bater). O agente escolhe entre essas duas a√ß√µes em cada passo.
- **Atualiza√ß√£o**: online por step (Q-learning TD update dentro do loop de tempo). Ou seja, `Q` e `V` s√£o atualizados imediatamente ap√≥s cada transi√ß√£o observada.
- **Discretiza√ß√£o atual**: observa√ß√µes cont√≠nuas s√£o arredondadas com uma casa decimal e transformadas em tupla, usada como chave nas tabelas:

```python
state = tuple(np.round(obs, 1))
```

Essa escolha √© simples e r√°pida, controlando a granularidade por n√∫mero de casas decimais. Menos casas ‚Üí menos estados; mais casas ‚Üí maior cardinalidade e sparsidade.
- **Reprodutibilidade e replay**: o c√≥digo gera um seed por epis√≥dio (usando o gerador NumPy para n√£o interferir com o gerador `random` da pol√≠tica), armazena a sequ√™ncia de a√ß√µes do melhor epis√≥dio e reproduz essa sequ√™ncia no final ‚Äî isto permite inspecionar qualitativamente o comportamento aprendido. üîÅ

## Hiperpar√¢metros relevantes ‚öôÔ∏è

- **Œ≥ (gamma)**: fator de desconto (ex.: `0.99`). Controla quanto o agente valoriza recompensas futuras.
- **Œ± (alpha)**: taxa de aprendizado (ex.: `0.1`). Controla o passo de atualiza√ß√£o do Q.
- **Œµ (epsilon)**: probabilidade inicial de explorar (ex.: `1.0`) com decaimento exponencial controlado por `epsilon_decay_rate` at√© `epsilon_end`.
- **Discretiza√ß√£o (precis√£o)**: definir quantas casas decimais usar em `np.round` √© uma escolha cr√≠tica ‚Äî impacta n√∫mero de estados e velocidade de aprendizagem.

## M√©tricas e an√°lise üìà

O projeto coleta e plota m√©tricas por epis√≥dio para avalia√ß√£o qualitativa:

- `scores`: n√∫mero de canos passados por epis√≥dio (interpretado como sucesso em curto prazo).
- `avg_rewards`: soma das recompensas recebidas no epis√≥dio (indicador de retorno bruto).
- `steps_per_episode`: sobreviv√™ncia (tempo) por epis√≥dio.
- `mean_V`: m√©dia dos valores `V(s)` conhecidos ‚Äî d√° uma ideia de converg√™ncia dos valores estimados.

Estas m√©tricas n√£o substituem uma an√°lise estat√≠stica robusta (v√°rios seeds independentes, intervalos de confian√ßa), mas ajudam a monitorar tend√™ncias e detectar regress√µes de desempenho. üìä

## Limita√ß√µes e suposi√ß√µes ‚ö†Ô∏è

- **Abordagem tabular**: funciona para espa√ßos de estado relativamente pequenos. Em observa√ß√µes cont√≠nuas com alta dimensionalidade, a tabela ser√° extremamente esparsa e o aprendizado lento.
- **Discretiza√ß√£o simples (arredondamento)**: f√°cil de implementar, mas sens√≠vel √† escala das vari√°veis. Se diferentes componentes da observa√ß√£o tiverem ordens de grandeza distintas, recomenda-se normaliza√ß√£o ou binning por dimens√£o.
- **Reprodutibilidade parcial**: o uso de `reset(seed=...)` tende a tornar o epis√≥dio inicial determin√≠stico, mas a reprodu√ß√£o perfeita depende de como o ambiente usa semente(s) internas.
- **Recompensas e sinal**: o design de recompensa do ambiente controla fortemente o aprendizado; aqui usamos as recompensas fornecidas pelo `flappy_bird_gymnasium` sem shaping adicional.

## Extens√µes e pr√≥ximas etapas sugeridas üå±

As seguintes melhorias s√£o naturais para transformar essa POC em uma linha de experimentos mais s√©ria:

- **Melhor discretiza√ß√£o**: substituir `np.round` por binning (com limites conhecidos por dimens√£o) ou por tile coding para melhores generaliza√ß√µes.
- **M√©todos funcionais**: migrar de tabelas para aproximadores (rede neural) e aplicar DQN ou pol√≠ticas ator-cr√≠tico para lidar com estados cont√≠nuos complexos.
- **Avalia√ß√£o estat√≠stica**: rodar m√∫ltiplos experimentos independentes (com diferentes seeds) e reportar m√©dia ¬± desvio padr√£o das m√©tricas.
- **Salvamento de checkpoints e exporta√ß√£o de runs (JSON)** para an√°lise offline e compara√ß√£o reproduz√≠vel.
- **Gera√ß√£o de v√≠deos dos replays** ou integra√ß√£o com ferramentas de visualiza√ß√£o para inspe√ß√£o qualitativa. üé•

## Leitura recomendada (conceitos) üìö

- *Sutton & Barto* ‚Äî _Reinforcement Learning: An Introduction_ (cap√≠tulos sobre MDPs, Monte Carlo, TD, Q-Learning)
- Tutoriais sobre discretiza√ß√£o: binning, tile coding e hashing para RL tabular
- Artigos e tutoriais sobre DQN e aproxima√ß√£o de fun√ß√£o para agentes em dom√≠nios com observa√ß√µes cont√≠nuas

---
